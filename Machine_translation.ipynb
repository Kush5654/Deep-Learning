{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE TRANSLATION USING DEEP LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing various libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushagra/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, \\\n",
    "  Bidirectional, RepeatVector, Concatenate, Activation, Dot, Lambda\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax overtime-It is the softmax function applied over timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_over_time(x):\n",
    "    \n",
    "    assert(K.ndim(x) > 2)\n",
    "    e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
    "    s = K.sum(e, axis=1, keepdims=True)\n",
    "    return e / s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imporing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples: 10000\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LATENT_DIM = 256\n",
    "LATENT_DIM_DECODER = 256 \n",
    "NUM_SAMPLES = 10000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_texts = [] \n",
    "target_texts = [] \n",
    "target_texts_inputs = [] \n",
    "\n",
    "\n",
    "\n",
    "t = 0\n",
    "for line in open('spa.txt'):\n",
    "    \n",
    "  \n",
    "    t += 1\n",
    "    if t > NUM_SAMPLES:\n",
    "        break\n",
    "\n",
    "  \n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "\n",
    "  \n",
    "    input_text, translation,_ = line.rstrip().split('\\t')\n",
    "\n",
    "\n",
    "    target_text = translation + ' <eos>'\n",
    "    target_text_input = '<sos> ' + translation\n",
    "\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    target_texts_inputs.append(target_text_input)\n",
    "print(\"num samples:\", len(input_texts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizing and Padding the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2337 unique input tokens.\n",
      "Found 6316 unique output tokens.\n",
      "encoder_data.shape: (10000, 5)\n",
      "encoder_data[0]: [ 0  0  0  0 15]\n",
      "decoder_data[0]: [   2 1468    0    0    0    0    0    0    0]\n",
      "decoder_data.shape: (10000, 9)\n"
     ]
    }
   ],
   "source": [
    "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
    "\n",
    "\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "print('Found %s unique input tokens.' % len(word2idx_inputs))\n",
    "\n",
    "\n",
    "max_len_input = max(len(s) for s in input_sequences)\n",
    "\n",
    "\n",
    "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) \n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n",
    "\n",
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "print('Found %s unique output tokens.' % len(word2idx_outputs))\n",
    "\n",
    "\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "\n",
    "\n",
    "max_len_target = max(len(s) for s in target_sequences)\n",
    "\n",
    "\n",
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
    "print(\"encoder_data.shape:\", encoder_inputs.shape)\n",
    "print(\"encoder_data[0]:\", encoder_inputs[0])\n",
    "\n",
    "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
    "print(\"decoder_data[0]:\", decoder_inputs[0])\n",
    "print(\"decoder_data.shape:\", decoder_inputs.shape)\n",
    "\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Loading Pre-trained wordembedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n",
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open('glove.6B/glove.6B.%sd.txt' % EMBEDDING_DIM) as f:\n",
    "\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "  if i < MAX_NUM_WORDS:\n",
    "    embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    " \n",
    "      embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=max_len_input,\n",
    "  \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "decoder_targets_one_hot = np.zeros(\n",
    "  (\n",
    "    len(input_texts),\n",
    "    max_len_target,\n",
    "    num_words_output\n",
    "  ),\n",
    "  dtype='float32'\n",
    ")\n",
    "\n",
    "\n",
    "for i, d in enumerate(decoder_targets):\n",
    "  for t, word in enumerate(d):\n",
    "    decoder_targets_one_hot[i, t, word] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining the Training architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = Bidirectional(LSTM(\n",
    "  LATENT_DIM,\n",
    "  return_sequences=True,\n",
    "\n",
    "))\n",
    "encoder_outputs = encoder(x)\n",
    "\n",
    "\n",
    "\n",
    "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
    "\n",
    "\n",
    "\n",
    "decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_repeat_layer = RepeatVector(max_len_input)\n",
    "attn_concat_layer = Concatenate(axis=-1)\n",
    "attn_dense1 = Dense(10, activation='tanh')\n",
    "attn_dense2 = Dense(1, activation=softmax_over_time)\n",
    "attn_dot = Dot(axes=1)\n",
    "\n",
    "def one_step_attention(h, st_1):\n",
    " \n",
    "  st_1 = attn_repeat_layer(st_1)\n",
    "\n",
    "\n",
    "  x = attn_concat_layer([h, st_1])\n",
    "\n",
    "\n",
    "  x = attn_dense1(x)\n",
    "\n",
    "  \n",
    "  alphas = attn_dense2(x)\n",
    "\n",
    "\n",
    "  context = attn_dot([alphas, h])\n",
    "\n",
    "  return context\n",
    "\n",
    "\n",
    "decoder_lstm = LSTM(LATENT_DIM_DECODER, return_state=True)\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "\n",
    "initial_s = Input(shape=(LATENT_DIM_DECODER,), name='s0')\n",
    "initial_c = Input(shape=(LATENT_DIM_DECODER,), name='c0')\n",
    "context_last_word_concat_layer = Concatenate(axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = initial_s\n",
    "c = initial_c\n",
    "\n",
    "\n",
    "outputs = []\n",
    "for t in range(max_len_target):\n",
    "  context = one_step_attention(encoder_outputs, s)\n",
    "\n",
    "\n",
    "  selector = Lambda(lambda x: x[:, t:t+1])\n",
    "  xt = selector(decoder_inputs_x)\n",
    "  \n",
    "\n",
    "  decoder_lstm_input = context_last_word_concat_layer([context, xt])\n",
    "\n",
    "\n",
    "  o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n",
    "\n",
    "\n",
    "  decoder_outputs = decoder_dense(o)\n",
    "  outputs.append(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack and Transpose is used to change the shape of the recieved output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_and_transpose(x):\n",
    "  x = K.stack(x) \n",
    "  x = K.permute_dimensions(x, pattern=(1, 0, 2)) \n",
    "  return x\n",
    "\n",
    "\n",
    "stacker = Lambda(stack_and_transpose)\n",
    "outputs = stacker(outputs)\n",
    "\n",
    "\n",
    "model = Model(\n",
    "  inputs=[\n",
    "    encoder_inputs_placeholder,\n",
    "    decoder_inputs_placeholder,\n",
    "    initial_s, \n",
    "    initial_c,\n",
    "  ],\n",
    "  outputs=outputs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 42s 5ms/step - loss: 2.7048 - acc: 0.6401 - val_loss: 2.6622 - val_acc: 0.6521\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 2.0480 - acc: 0.7133 - val_loss: 2.4673 - val_acc: 0.6671\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 1.8318 - acc: 0.7305 - val_loss: 2.2700 - val_acc: 0.6819\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 1.6755 - acc: 0.7486 - val_loss: 2.1702 - val_acc: 0.6996\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 1.5400 - acc: 0.7649 - val_loss: 2.0834 - val_acc: 0.7147\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 1.4282 - acc: 0.7778 - val_loss: 2.0431 - val_acc: 0.7233\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 1.3278 - acc: 0.7888 - val_loss: 1.9736 - val_acc: 0.7304\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 1.2399 - acc: 0.8004 - val_loss: 1.9613 - val_acc: 0.7359\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 1.1678 - acc: 0.8095 - val_loss: 1.9216 - val_acc: 0.7416\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 1.0989 - acc: 0.8192 - val_loss: 1.9105 - val_acc: 0.7416\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 1.0397 - acc: 0.8273 - val_loss: 1.9153 - val_acc: 0.7451\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.9831 - acc: 0.8351 - val_loss: 1.9073 - val_acc: 0.7466\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.9274 - acc: 0.8422 - val_loss: 1.9071 - val_acc: 0.7469\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.8783 - acc: 0.8493 - val_loss: 1.8895 - val_acc: 0.7494\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.8315 - acc: 0.8573 - val_loss: 1.9163 - val_acc: 0.7457\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.7928 - acc: 0.8637 - val_loss: 1.9373 - val_acc: 0.7456\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.7581 - acc: 0.8693 - val_loss: 1.9435 - val_acc: 0.7433\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.7181 - acc: 0.8761 - val_loss: 1.9534 - val_acc: 0.7444\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.6760 - acc: 0.8811 - val_loss: 1.9478 - val_acc: 0.7389\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.6421 - acc: 0.8855 - val_loss: 1.9460 - val_acc: 0.7388\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.6110 - acc: 0.8906 - val_loss: 1.9708 - val_acc: 0.7398\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.5809 - acc: 0.8959 - val_loss: 1.9573 - val_acc: 0.7395\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.5530 - acc: 0.8994 - val_loss: 1.9880 - val_acc: 0.7378\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 25s 3ms/step - loss: 0.5291 - acc: 0.9027 - val_loss: 1.9897 - val_acc: 0.7379\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 25s 3ms/step - loss: 0.5100 - acc: 0.9059 - val_loss: 2.0276 - val_acc: 0.7358\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.4880 - acc: 0.9098 - val_loss: 2.0306 - val_acc: 0.7346\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.4665 - acc: 0.9132 - val_loss: 2.0581 - val_acc: 0.7361\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 23s 3ms/step - loss: 0.4498 - acc: 0.9160 - val_loss: 2.0842 - val_acc: 0.7330\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.4325 - acc: 0.9188 - val_loss: 2.0953 - val_acc: 0.7354\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.4168 - acc: 0.9219 - val_loss: 2.1098 - val_acc: 0.7343\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.4021 - acc: 0.9231 - val_loss: 2.0958 - val_acc: 0.7359\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.3852 - acc: 0.9260 - val_loss: 2.1373 - val_acc: 0.7322\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.3715 - acc: 0.9291 - val_loss: 2.1284 - val_acc: 0.7328\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.3615 - acc: 0.9303 - val_loss: 2.1464 - val_acc: 0.7318\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.3508 - acc: 0.9319 - val_loss: 2.1609 - val_acc: 0.7308\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.3393 - acc: 0.9344 - val_loss: 2.1789 - val_acc: 0.7321\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.3290 - acc: 0.9361 - val_loss: 2.2025 - val_acc: 0.7312\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.3188 - acc: 0.9379 - val_loss: 2.1961 - val_acc: 0.7307\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.3087 - acc: 0.9389 - val_loss: 2.2123 - val_acc: 0.7304\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.2999 - acc: 0.9405 - val_loss: 2.2232 - val_acc: 0.7296\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.2920 - acc: 0.9411 - val_loss: 2.2335 - val_acc: 0.7296\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2826 - acc: 0.9427 - val_loss: 2.2505 - val_acc: 0.7286\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 20s 2ms/step - loss: 0.2747 - acc: 0.9429 - val_loss: 2.2542 - val_acc: 0.7294\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2664 - acc: 0.9440 - val_loss: 2.2504 - val_acc: 0.7312\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2570 - acc: 0.9455 - val_loss: 2.2690 - val_acc: 0.7280\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2491 - acc: 0.9462 - val_loss: 2.2974 - val_acc: 0.7271\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2413 - acc: 0.9467 - val_loss: 2.2840 - val_acc: 0.7276\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2352 - acc: 0.9484 - val_loss: 2.2946 - val_acc: 0.7287\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2290 - acc: 0.9482 - val_loss: 2.2960 - val_acc: 0.7286\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2234 - acc: 0.9494 - val_loss: 2.2994 - val_acc: 0.7281\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2185 - acc: 0.9497 - val_loss: 2.3102 - val_acc: 0.7272\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2141 - acc: 0.9498 - val_loss: 2.3169 - val_acc: 0.7268\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2098 - acc: 0.9504 - val_loss: 2.3296 - val_acc: 0.7278\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2052 - acc: 0.9504 - val_loss: 2.3365 - val_acc: 0.7267\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2015 - acc: 0.9509 - val_loss: 2.3410 - val_acc: 0.7263\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1972 - acc: 0.9516 - val_loss: 2.3392 - val_acc: 0.7279\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1939 - acc: 0.9518 - val_loss: 2.3574 - val_acc: 0.7261\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1890 - acc: 0.9522 - val_loss: 2.3621 - val_acc: 0.7283\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1862 - acc: 0.9523 - val_loss: 2.3771 - val_acc: 0.7267\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1830 - acc: 0.9528 - val_loss: 2.3696 - val_acc: 0.7273\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1794 - acc: 0.9530 - val_loss: 2.3795 - val_acc: 0.7292\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1748 - acc: 0.9531 - val_loss: 2.3948 - val_acc: 0.7269\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1721 - acc: 0.9528 - val_loss: 2.3900 - val_acc: 0.7281\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1688 - acc: 0.9535 - val_loss: 2.3974 - val_acc: 0.7264\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1660 - acc: 0.9533 - val_loss: 2.4176 - val_acc: 0.7259\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1625 - acc: 0.9539 - val_loss: 2.4196 - val_acc: 0.7266\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1603 - acc: 0.9535 - val_loss: 2.4131 - val_acc: 0.7252\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1574 - acc: 0.9536 - val_loss: 2.4222 - val_acc: 0.7256\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1553 - acc: 0.9537 - val_loss: 2.4331 - val_acc: 0.7266\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1524 - acc: 0.9536 - val_loss: 2.4301 - val_acc: 0.7253\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1489 - acc: 0.9543 - val_loss: 2.4363 - val_acc: 0.7269\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1466 - acc: 0.9549 - val_loss: 2.4461 - val_acc: 0.7271\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1447 - acc: 0.9542 - val_loss: 2.4444 - val_acc: 0.7264\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1436 - acc: 0.9542 - val_loss: 2.4579 - val_acc: 0.7261\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1418 - acc: 0.9540 - val_loss: 2.4537 - val_acc: 0.7283\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1405 - acc: 0.9541 - val_loss: 2.4576 - val_acc: 0.7290\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1393 - acc: 0.9543 - val_loss: 2.4634 - val_acc: 0.7271\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1376 - acc: 0.9540 - val_loss: 2.4753 - val_acc: 0.7271\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1367 - acc: 0.9543 - val_loss: 2.4847 - val_acc: 0.7275\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1357 - acc: 0.9544 - val_loss: 2.4870 - val_acc: 0.7279\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1343 - acc: 0.9540 - val_loss: 2.4852 - val_acc: 0.7257\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1336 - acc: 0.9535 - val_loss: 2.4871 - val_acc: 0.7261\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1324 - acc: 0.9546 - val_loss: 2.4996 - val_acc: 0.7272\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1314 - acc: 0.9542 - val_loss: 2.4936 - val_acc: 0.7283\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1308 - acc: 0.9542 - val_loss: 2.5129 - val_acc: 0.7257\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1295 - acc: 0.9538 - val_loss: 2.5099 - val_acc: 0.7270\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1293 - acc: 0.9543 - val_loss: 2.5081 - val_acc: 0.7268\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1279 - acc: 0.9540 - val_loss: 2.5296 - val_acc: 0.7262\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1273 - acc: 0.9541 - val_loss: 2.5306 - val_acc: 0.7257\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 20s 2ms/step - loss: 0.1267 - acc: 0.9539 - val_loss: 2.5355 - val_acc: 0.7274\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1260 - acc: 0.9540 - val_loss: 2.5319 - val_acc: 0.7266\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1250 - acc: 0.9540 - val_loss: 2.5313 - val_acc: 0.7288\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1246 - acc: 0.9535 - val_loss: 2.5609 - val_acc: 0.7257\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1237 - acc: 0.9536 - val_loss: 2.5271 - val_acc: 0.7284\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1231 - acc: 0.9536 - val_loss: 2.5680 - val_acc: 0.7258\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1225 - acc: 0.9538 - val_loss: 2.5463 - val_acc: 0.7276\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1220 - acc: 0.9533 - val_loss: 2.5637 - val_acc: 0.7269\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1209 - acc: 0.9536 - val_loss: 2.5605 - val_acc: 0.7276\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1205 - acc: 0.9541 - val_loss: 2.5597 - val_acc: 0.7282\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 21s 3ms/step - loss: 0.1197 - acc: 0.9541 - val_loss: 2.5843 - val_acc: 0.7270\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "z = np.zeros((NUM_SAMPLES, LATENT_DIM_DECODER)) \n",
    "r = model.fit(\n",
    "  [encoder_inputs, decoder_inputs, z, z], decoder_targets_one_hot,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prediction Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs_placeholder, encoder_outputs)\n",
    "\n",
    "\n",
    "encoder_outputs_as_input = Input(shape=(max_len_input, LATENT_DIM * 2,))\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "\n",
    "\n",
    "context = one_step_attention(encoder_outputs_as_input, initial_s)\n",
    "\n",
    "decoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])\n",
    "\n",
    "\n",
    "o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
    "decoder_outputs = decoder_dense(o)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create the model object\n",
    "decoder_model = Model(\n",
    "  inputs=[\n",
    "    decoder_inputs_single,\n",
    "    encoder_outputs_as_input,\n",
    "    initial_s, \n",
    "    initial_c\n",
    "  ],\n",
    "  outputs=[decoder_outputs, s, c]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicting the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "\n",
    "  enc_out = encoder_model.predict(input_seq)\n",
    "\n",
    "\n",
    "  target_seq = np.zeros((1, 1))\n",
    "  \n",
    "\n",
    "  target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "\n",
    "\n",
    "  eos = word2idx_outputs['<eos>']\n",
    "\n",
    "\n",
    "  s = np.zeros((1, LATENT_DIM_DECODER))\n",
    "  c = np.zeros((1, LATENT_DIM_DECODER))\n",
    "\n",
    "\n",
    "\n",
    "  output_sentence = []\n",
    "  for _ in range(max_len_target):\n",
    "    o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n",
    "        \n",
    "\n",
    "\n",
    "    idx = np.argmax(o.flatten())\n",
    "\n",
    "   \n",
    "    if eos == idx:\n",
    "      break\n",
    "\n",
    "    word = ''\n",
    "    if idx > 0:\n",
    "      word = idx2word_trans[idx]\n",
    "      output_sentence.append(word)\n",
    "\n",
    "\n",
    "    target_seq[0, 0] = idx\n",
    "\n",
    "  return ' '.join(output_sentence)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Live Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: I have no idea.\n",
      "Predicted translation: no tengo idea.\n",
      "Actual translation: Ni idea. <eos>\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input sentence: I imagined that.\n",
      "Predicted translation: a mí eso.\n",
      "Actual translation: Me imaginé eso. <eos>\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    " \n",
    "  i = np.random.choice(len(input_texts))\n",
    "  input_seq = encoder_inputs[i:i+1]\n",
    "  translation = decode_sequence(input_seq)\n",
    "  print('-')\n",
    "  print('Input sentence:', input_texts[i])\n",
    "  print('Predicted translation:', translation)\n",
    "  print('Actual translation:', target_texts[i])\n",
    "\n",
    "  ans = input(\"Continue? [Y/n]\")\n",
    "  if ans and ans.lower().startswith('n'):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
